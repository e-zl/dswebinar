{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZxC8jYc6HHT"
   },
   "source": [
    "# PySpark DataFrames and SQL\n",
    "\n",
    "[Jian Tao](https://orcid.org/0000-0003-4228-6089), Texas A&M University\n",
    "\n",
    "May 1, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the PySpark environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4163,
     "status": "ok",
     "timestamp": 1619691257105,
     "user": {
      "displayName": "Jian Tao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhG9Pe7fCrlHe_UyoLCrFepsZ6ZnpuXvpiT4aLR=s64",
      "userId": "01925615186988906375"
     },
     "user_tz": 300
    },
    "id": "WJlWGm4aEZeM",
    "outputId": "8891d94c-1a3a-4b82-e31d-338658cb3993"
   },
   "outputs": [],
   "source": [
    "# For each Google Colab, we will need to run this cell to ensure that PySpark is installed properly.\n",
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip -o ngrok-stable-linux-amd64.zip\n",
    "# get_ipython().system_raw('./ngrok http 4050 &')\n",
    "# !curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(\\\"\\nClick me to launch (give it a minute or two)\\n\\\"); print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a DataFrame by reading from a CSV/JSON file\n",
    "\n",
    "`spark.read.csv` can only read from local files, so we will have to download the CSV file from the URL first. We can use `SparkFiles` to do that or use `pandas`. For those CSV files with a header, please make sure to set `header=True` in the argument list for `spark.read.csv`. When the data types of the columns are not known, `inferSchema=True` will do the trick to automatically recognize the data types, but it is not perfect. In our example, `Horsepower` is not correctly recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6923,
     "status": "ok",
     "timestamp": 1599793039176,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "8VhDbsJr64Xh",
    "outputId": "783d9bd6-85dd-45d2-e632-28011e6f1e59"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "csv_url = \"https://raw.githubusercontent.com/jtao/AdvancedML/main/data/Auto.csv\"\n",
    "json_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.json\"\n",
    "\n",
    "spark.sparkContext.addFile(csv_url)\n",
    "spark.sparkContext.addFile(json_url)\n",
    "\n",
    "## One can create a spark dataframe from pandas dataframe as well.\n",
    "# import pandas as pd\n",
    "# df = spark.createDataFrame(pd.read_csv(url))\n",
    "\n",
    "#df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=False)\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either will work\n",
    "df = spark.read.json(SparkFiles.get(\"Auto.json\"))\n",
    "#df = spark.read.load(SparkFiles.get(\"Auto.json\"),format=\"json\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a schema to help `spark.read.csv` to correctly cast the type of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1599793255822,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "SLJ4ZWApDCbS",
    "outputId": "2ed2fa86-e497-449d-eb98-1a076f6cb79a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "user_schema = StructType([\n",
    "                     StructField(\"mpg\", IntegerType(), True),\n",
    "                     StructField(\"cylinders\", IntegerType(), True),\n",
    "                     StructField(\"displacement\", IntegerType(), True),\n",
    "                     StructField(\"horsepower\", IntegerType(), True),\n",
    "                     StructField(\"weight\", IntegerType(), True),\n",
    "                     StructField(\"acceleration\", DoubleType(), True),\n",
    "                     StructField(\"year\", IntegerType(), True),\n",
    "                     StructField(\"origin\", IntegerType(), True),\n",
    "                     StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", schema=user_schema, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(SparkFiles.get(\"Auto.json\"))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Spark DataFrame with a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_list = [(1, 18, \"Chevrolet\"), (2, 15, \"Buick\"), (3, 18, \"Plymouth\"), (4, 16, \"Amc\"), (5, 17, \"Ford\")]\n",
    "\n",
    "df = spark.createDataFrame(auto_list)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "users_schema = StructType([\n",
    "                          StructField(\"id\", IntegerType(), True),\n",
    "                          StructField(\"mpg\", IntegerType(), True),\n",
    "                          StructField(\"name\", StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(auto_list, schema=users_schema)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Spark DataFrame with a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_list = [{\"id\": 1, \"mpg\": 18, \"name\": \"Chevrolet\"}, \n",
    "                {\"id\": 2, \"mpg\": 15, \"name\": \"Buick\"}, \n",
    "                {\"id\": 3, \"mpg\": 18, \"name\": \"Plymouth\"}, \n",
    "                {\"id\": 4, \"mpg\": 16, \"name\": \"Amc\"}, \n",
    "                {\"id\": 5, \"mpg\": 17, \"name\": \"Ford\"}]\n",
    "df = spark.createDataFrame(auto_list)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Operations on Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full data set again.\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select everybody, but increment the mpg by 100\n",
    "df.select(df['name'], df['mpg'] + 100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select mpg greater than 30\n",
    "df.filter(df['mpg'] > 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Cars by cylinders\n",
    "df.groupBy(\"cylinders\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Running SQL queries programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"auto\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM auto\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.auto\").show(5)\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.auto\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spark, one can run SQL queries directly on files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM json.`Auto.json`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. References:\n",
    "\n",
    "SQL References\n",
    "https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOn5O632gStYnUc89z/d1/B",
   "collapsed_sections": [],
   "name": "Create DataFrame from CSV File in PySpark 3.0 on Google Colab | Part 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
