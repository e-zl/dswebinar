{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZxC8jYc6HHT"
   },
   "source": [
    "# PySpark DataFrames and SQL\n",
    "\n",
    "[Jian Tao](https://orcid.org/0000-0003-4228-6089), Texas A&M University\n",
    "\n",
    "May 1, 2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the PySpark environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4163,
     "status": "ok",
     "timestamp": 1619691257105,
     "user": {
      "displayName": "Jian Tao",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhG9Pe7fCrlHe_UyoLCrFepsZ6ZnpuXvpiT4aLR=s64",
      "userId": "01925615186988906375"
     },
     "user_tz": 300
    },
    "id": "WJlWGm4aEZeM",
    "outputId": "8891d94c-1a3a-4b82-e31d-338658cb3993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/jtao/Projects/venv/lib/python3.8/site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/jtao/Projects/venv/lib/python3.8/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "# For each Google Colab, we will need to run this cell to ensure that PySpark is installed properly.\n",
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()\n",
    "\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip -o ngrok-stable-linux-amd64.zip\n",
    "# get_ipython().system_raw('./ngrok http 4050 &')\n",
    "# !curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(\\\"\\nClick me to launch (give it a minute or two)\\n\\\"); print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a DataFrame by reading from a CSV/JSON file\n",
    "\n",
    "`spark.read.csv` can only read from local files, so we will have to download the CSV file from the URL first. We can use `SparkFiles` to do that or use `pandas`. For those CSV files with a header, please make sure to set `header=True` in the argument list for `spark.read.csv`. When the data types of the columns are not known, `inferSchema=True` will do the trick to automatically recognize the data types, but it is not perfect. In our example, `Horsepower` is not correctly recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6923,
     "status": "ok",
     "timestamp": 1599793039176,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "8VhDbsJr64Xh",
    "outputId": "783d9bd6-85dd-45d2-e632-28011e6f1e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mpg: double (nullable = true)\n",
      " |-- cylinders: integer (nullable = true)\n",
      " |-- displacement: double (nullable = true)\n",
      " |-- horsepower: string (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- acceleration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- origin: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|year|origin|                name|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "|18.0|        8|       307.0|       130|  3504|        12.0|  70|     1|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|       165|  3693|        11.5|  70|     1|   buick skylark 320|\n",
      "|18.0|        8|       318.0|       150|  3436|        11.0|  70|     1|  plymouth satellite|\n",
      "|16.0|        8|       304.0|       150|  3433|        12.0|  70|     1|       amc rebel sst|\n",
      "|17.0|        8|       302.0|       140|  3449|        10.5|  70|     1|         ford torino|\n",
      "+----+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "csv_url = \"https://raw.githubusercontent.com/jtao/AdvancedML/main/data/Auto.csv\"\n",
    "json_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.json\"\n",
    "\n",
    "spark.sparkContext.addFile(csv_url)\n",
    "spark.sparkContext.addFile(json_url)\n",
    "\n",
    "## One can create a spark dataframe from pandas dataframe as well.\n",
    "# import pandas as pd\n",
    "# df = spark.createDataFrame(pd.read_csv(url))\n",
    "\n",
    "#df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=False)\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(SparkFiles.get(\"Auto.json\"), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        acceleration|           cylinders|        displacement|          horsepower|                 mpg|                name|              origin|              weight|                year|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|{12.0, 11.5, 10.0...|{8, 8, 8, 6, 6, 4...|{307.0, 350.0, 38...|{130, 165, 170, 8...|{18.0, 15.0, 15.0...|{chevrolet chevel...|{1, 1, 1, 1, 1, 2...|{3504, 3693, 3563...|{70, 70, 70, 73, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a schema to help `spark.read.csv` to correctly cast the type of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1599793255822,
     "user": {
      "displayName": "DataMaking",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8Ck4eyinapgtobtrErZWzNt_3DfJL867BQ7JA-Q=s64",
      "userId": "16773646856361708024"
     },
     "user_tz": -330
    },
    "id": "SLJ4ZWApDCbS",
    "outputId": "2ed2fa86-e497-449d-eb98-1a076f6cb79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mpg: integer (nullable = true)\n",
      " |-- cylinders: integer (nullable = true)\n",
      " |-- displacement: integer (nullable = true)\n",
      " |-- horsepower: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- acceleration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- origin: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "|mpg|cylinders|displacement|horsepower|weight|acceleration|year|origin|                name|\n",
      "+---+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "| 18|        8|         307|       130|  3504|        12.0|  70|     1|chevrolet chevell...|\n",
      "| 15|        8|         350|       165|  3693|        11.5|  70|     1|   buick skylark 320|\n",
      "| 18|        8|         318|       150|  3436|        11.0|  70|     1|  plymouth satellite|\n",
      "| 16|        8|         304|       150|  3433|        12.0|  70|     1|       amc rebel sst|\n",
      "| 17|        8|         302|       140|  3449|        10.5|  70|     1|         ford torino|\n",
      "+---+---------+------------+----------+------+------------+----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "user_schema = StructType([\n",
    "                     StructField(\"mpg\", IntegerType(), True),\n",
    "                     StructField(\"cylinders\", IntegerType(), True),\n",
    "                     StructField(\"displacement\", IntegerType(), True),\n",
    "                     StructField(\"horsepower\", IntegerType(), True),\n",
    "                     StructField(\"weight\", IntegerType(), True),\n",
    "                     StructField(\"acceleration\", DoubleType(), True),\n",
    "                     StructField(\"year\", IntegerType(), True),\n",
    "                     StructField(\"origin\", IntegerType(), True),\n",
    "                     StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", schema=user_schema, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Spark DataFrame with a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n",
      "+---+---+---------+\n",
      "| _1| _2|       _3|\n",
      "+---+---+---------+\n",
      "|  1| 18|Chevrolet|\n",
      "|  2| 15|    Buick|\n",
      "|  3| 18| Plymouth|\n",
      "|  4| 16|      Amc|\n",
      "|  5| 17|     Ford|\n",
      "+---+---+---------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- mpg: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+---+---------+\n",
      "| id|mpg|     name|\n",
      "+---+---+---------+\n",
      "|  1| 18|Chevrolet|\n",
      "|  2| 15|    Buick|\n",
      "|  3| 18| Plymouth|\n",
      "|  4| 16|      Amc|\n",
      "|  5| 17|     Ford|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auto_list = [(1, 18, \"Chevrolet\"), (2, 15, \"Buick\"), (3, 18, \"Plymouth\"), (4, 16, \"Amc\"), (5, 17, \"Ford\")]\n",
    "\n",
    "df = spark.createDataFrame(auto_list)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "users_schema = StructType([\n",
    "                          StructField(\"id\", IntegerType(), True),\n",
    "                          StructField(\"mpg\", IntegerType(), True),\n",
    "                          StructField(\"name\", StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(auto_list, schema=users_schema)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Spark DataFrame with a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- mpg: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+---+---------+\n",
      "| id|mpg|     name|\n",
      "+---+---+---------+\n",
      "|  1| 18|Chevrolet|\n",
      "|  2| 15|    Buick|\n",
      "|  3| 18| Plymouth|\n",
      "|  4| 16|      Amc|\n",
      "|  5| 17|     Ford|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auto_list = [{\"id\": 1, \"mpg\": 18, \"name\": \"Chevrolet\"}, \n",
    "                {\"id\": 2, \"mpg\": 15, \"name\": \"Buick\"}, \n",
    "                {\"id\": 3, \"mpg\": 18, \"name\": \"Plymouth\"}, \n",
    "                {\"id\": 4, \"mpg\": 16, \"name\": \"Amc\"}, \n",
    "                {\"id\": 5, \"mpg\": 17, \"name\": \"Ford\"}]\n",
    "df = spark.createDataFrame(auto_list)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"Auto.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cwvaa5V9DYK"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOn5O632gStYnUc89z/d1/B",
   "collapsed_sections": [],
   "name": "Create DataFrame from CSV File in PySpark 3.0 on Google Colab | Part 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
