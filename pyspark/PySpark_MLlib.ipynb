{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtao/dswebinar/blob/master/pyspark/PySpark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZxC8jYc6HHT"
      },
      "source": [
        "# PySpark DataFrames and SQL\n",
        "\n",
        "[Jian Tao](https://orcid.org/0000-0003-4228-6089), Texas A&M University\n",
        "\n",
        "May 1, 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPELjsLYhub-"
      },
      "source": [
        "### 1. Set up the PySpark environment first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJlWGm4aEZeM"
      },
      "outputs": [],
      "source": [
        "# For each Google Colab, we will need to run this cell to ensure that PySpark is installed properly.\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Colab\").config('spark.ui.port', '4050').getOrCreate()\n",
        "\n",
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "# !unzip -o ngrok-stable-linux-amd64.zip\n",
        "# get_ipython().system_raw('./ngrok http 4050 &')\n",
        "# !curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(\\\"\\nClick me to launch (give it a minute or two)\\n\\\"); print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQhDvdKrhucB"
      },
      "source": [
        "### 2. Create a DataFrame by reading from a CSV/JSON file\n",
        "\n",
        "`spark.read.csv` can only read from local files, so we will have to download the CSV file from the URL first. We can use `SparkFiles` to do that or use `pandas`. For those CSV files with a header, please make sure to set `header=True` in the argument list for `spark.read.csv`. When the data types of the columns are not known, `inferSchema=True` will do the trick to automatically recognize the data types, but it is not perfect. In our example, `Horsepower` is not correctly recognized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VhDbsJr64Xh"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkFiles\n",
        "\n",
        "csv_url = \"https://raw.githubusercontent.com/jtao/AdvancedML/main/data/Auto.csv\"\n",
        "json_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.json\"\n",
        "\n",
        "spark.sparkContext.addFile(csv_url)\n",
        "spark.sparkContext.addFile(json_url)\n",
        "\n",
        "## One can create a spark dataframe from pandas dataframe as well.\n",
        "# import pandas as pd\n",
        "# df = spark.createDataFrame(pd.read_csv(url))\n",
        "\n",
        "#df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=False)\n",
        "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ09OixahucC"
      },
      "outputs": [],
      "source": [
        "# either will work\n",
        "df = spark.read.json(SparkFiles.get(\"Auto.json\"))\n",
        "#df = spark.read.load(SparkFiles.get(\"Auto.json\"),format=\"json\")\n",
        "\n",
        "df.printSchema()\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Fqlr2ChucD"
      },
      "source": [
        "We can define a schema to help `spark.read.csv` to correctly cast the type of all the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLJ4ZWApDCbS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "user_schema = StructType([\n",
        "                     StructField(\"mpg\", IntegerType(), True),\n",
        "                     StructField(\"cylinders\", IntegerType(), True),\n",
        "                     StructField(\"displacement\", IntegerType(), True),\n",
        "                     StructField(\"horsepower\", IntegerType(), True),\n",
        "                     StructField(\"weight\", IntegerType(), True),\n",
        "                     StructField(\"acceleration\", DoubleType(), True),\n",
        "                     StructField(\"year\", IntegerType(), True),\n",
        "                     StructField(\"origin\", IntegerType(), True),\n",
        "                     StructField(\"name\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", schema=user_schema, inferSchema=True)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frkvfuQDhucE"
      },
      "outputs": [],
      "source": [
        "df = spark.read.json(SparkFiles.get(\"Auto.json\"))\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQWsbwjHhucF"
      },
      "source": [
        "### 3. Create a Spark DataFrame with a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEn17c7dhucF"
      },
      "outputs": [],
      "source": [
        "auto_list = [(1, 18, \"Chevrolet\"), (2, 15, \"Buick\"), (3, 18, \"Plymouth\"), (4, 16, \"Amc\"), (5, 17, \"Ford\")]\n",
        "\n",
        "df = spark.createDataFrame(auto_list)\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "users_schema = StructType([\n",
        "                          StructField(\"id\", IntegerType(), True),\n",
        "                          StructField(\"mpg\", IntegerType(), True),\n",
        "                          StructField(\"name\", StringType(), True)])\n",
        "\n",
        "df = spark.createDataFrame(auto_list, schema=users_schema)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD68fXq5hucF"
      },
      "source": [
        "### 3. Create a Spark DataFrame with a list of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UpVtzFOhucG"
      },
      "outputs": [],
      "source": [
        "auto_list = [{\"id\": 1, \"mpg\": 18, \"name\": \"Chevrolet\"}, \n",
        "                {\"id\": 2, \"mpg\": 15, \"name\": \"Buick\"}, \n",
        "                {\"id\": 3, \"mpg\": 18, \"name\": \"Plymouth\"}, \n",
        "                {\"id\": 4, \"mpg\": 16, \"name\": \"Amc\"}, \n",
        "                {\"id\": 5, \"mpg\": 17, \"name\": \"Ford\"}]\n",
        "df = spark.createDataFrame(auto_list)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Ams7oOhucG"
      },
      "source": [
        "### 4. Operations on Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGZ7R6umhucG"
      },
      "outputs": [],
      "source": [
        "# Load the full data set again.\n",
        "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIxQHSiHhucG"
      },
      "outputs": [],
      "source": [
        "# Select only the \"name\" column\n",
        "df.select(\"name\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my-3_ysUhucH"
      },
      "outputs": [],
      "source": [
        "# Select everybody, but increment the mpg by 100\n",
        "df.select(df['name'], df['mpg'] + 100).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2pqOBBYhucH"
      },
      "outputs": [],
      "source": [
        "# Select mpg greater than 30\n",
        "df.filter(df['mpg'] > 30).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbMHKWLyhucH"
      },
      "outputs": [],
      "source": [
        "# Count Cars by cylinders\n",
        "df.groupBy(\"cylinders\").count().show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEpoRDsmhucH"
      },
      "source": [
        "### 5. Running SQL queries programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VwQVMg2hucH"
      },
      "outputs": [],
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createOrReplaceTempView(\"auto\")\n",
        "sqlDF = spark.sql(\"SELECT * FROM auto\")\n",
        "sqlDF.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBFhc90lhucI"
      },
      "outputs": [],
      "source": [
        "# Register the DataFrame as a global temporary view\n",
        "df.createGlobalTempView(\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URegWil3hucI"
      },
      "outputs": [],
      "source": [
        "# Global temporary view is tied to a system preserved database `global_temp`\n",
        "spark.sql(\"SELECT * FROM global_temp.auto\").show(5)\n",
        "\n",
        "# Global temporary view is cross-session\n",
        "spark.newSession().sql(\"SELECT * FROM global_temp.auto\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hSM0R7HhucI"
      },
      "source": [
        "With spark, one can run SQL queries directly on files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xzCN6xRhucI"
      },
      "outputs": [],
      "source": [
        "df = spark.sql(\"SELECT * FROM json.`Auto.json`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUw486dchucJ"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z_EqJ3bhucJ"
      },
      "source": [
        "### 6. References:\n",
        "\n",
        "SQL References\n",
        "https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PySpark MLlib.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}