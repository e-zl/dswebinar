{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_with_NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtao/dswebinar/blob/master/nlp/NLP_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMYbTJGjtMoz"
      },
      "source": [
        "# Natural Language Processing with NLTK\n",
        "\n",
        "[Jian Tao](https://coehpc.engr.tamu.edu/people/jian-tao/), Texas A&M University\n",
        "\n",
        "Sept 17, 2021\n",
        "\n",
        "Converted from \n",
        "\n",
        "**Intro to natural language processing with Python**\n",
        "\n",
        "Notebook by [Juan Cruz Martinez](https://livecodestream.dev/authors/bajcmartinez/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfKt9ernTGcK"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5aFgDKqOBI"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JpHHRXTKrI"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwgHiH5qTQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fdd81c-4674-4fdc-9cd2-9f03db3054a7"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAtuBOHr0Hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88931706-6c8e-4e74-82bf-0bcfea442833"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "Text = \"Good morning, How you doing? Are you coming tonight?\"\n",
        "Tokenized = word_tokenize(Text)\n",
        "print(Tokenized)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'morning', ',', 'How', 'you', 'doing', '?', 'Are', 'you', 'coming', 'tonight', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZavEyf3shdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229759d6-b8b4-4241-c2f9-8dacec87fd2a"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "Text = \"Good morning, How you doing? Are you coming tonight?\"\n",
        "Tokenized = sent_tokenize(Text)\n",
        "print(Tokenized)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning, How you doing?', 'Are you coming tonight?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug23r1ziTRBB"
      },
      "source": [
        "## Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_X7WZCtvSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd10b36d-2659-4e08-f444-e1c4070d9eb8"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFPhwmAt0_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c55171-05a4-4b5b-fba3-4202e834537e"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "Text = [\"Good\", \"morning\", \"How\", \"you\", \"doing\", \"Are\", \"you\", \"coming\", \"tonight\"]\n",
        "for i in Text:\n",
        "   if i not in stopwords:\n",
        "       print(i)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good\n",
            "morning\n",
            "How\n",
            "Are\n",
            "coming\n",
            "tonight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbC3CKm-vTHs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "8877c2a2-19f2-46ee-a63f-a5e60663a2d9"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "','.join(stopwords)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i,me,my,myself,we,our,ours,ourselves,you,you're,you've,you'll,you'd,your,yours,yourself,yourselves,he,him,his,himself,she,she's,her,hers,herself,it,it's,its,itself,they,them,their,theirs,themselves,what,which,who,whom,this,that,that'll,these,those,am,is,are,was,were,be,been,being,have,has,had,having,do,does,did,doing,a,an,the,and,but,if,or,because,as,until,while,of,at,by,for,with,about,against,between,into,through,during,before,after,above,below,to,from,up,down,in,out,on,off,over,under,again,further,then,once,here,there,when,where,why,how,all,any,both,each,few,more,most,other,some,such,no,nor,not,only,own,same,so,than,too,very,s,t,can,will,just,don,don't,should,should've,now,d,ll,m,o,re,ve,y,ain,aren,aren't,couldn,couldn't,didn,didn't,doesn,doesn't,hadn,hadn't,hasn,hasn't,haven,haven't,isn,isn't,ma,mightn,mightn't,mustn,mustn't,needn,needn't,shan,shan't,shouldn,shouldn't,wasn,wasn't,weren,weren't,won,won't,wouldn,wouldn't\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDmzpqRTUdn"
      },
      "source": [
        "## Stemming Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI2Olf5asdTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f5f5d5-98ff-4d11-b9f9-9fe318090c06"
      },
      "source": [
        "help(nltk.stem)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package nltk.stem in nltk:\n",
            "\n",
            "NAME\n",
            "    nltk.stem - NLTK Stemmers\n",
            "\n",
            "DESCRIPTION\n",
            "    Interfaces used to remove morphological affixes from words, leaving\n",
            "    only the word stem.  Stemming algorithms aim to remove those affixes\n",
            "    required for eg. grammatical role, tense, derivational morphology\n",
            "    leaving only the stem of the word.  This is a difficult problem due to\n",
            "    irregular words (eg. common verbs in English), complicated\n",
            "    morphological rules, and part-of-speech and sense ambiguities\n",
            "    (eg. ``ceil-`` is not the stem of ``ceiling``).\n",
            "    \n",
            "    StemmerI defines a standard interface for stemmers.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    api\n",
            "    arlstem\n",
            "    arlstem2\n",
            "    cistem\n",
            "    isri\n",
            "    lancaster\n",
            "    porter\n",
            "    regexp\n",
            "    rslp\n",
            "    snowball\n",
            "    util\n",
            "    wordnet\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2JbM81u0RJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3a57e7-efc2-44a8-e377-180aa3cd2dd5"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"Loving\", \"Chocolate\", \"Retrieved\"]\n",
        "for i in words:\n",
        "   print(ps.stem(i))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love\n",
            "chocol\n",
            "retriev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBjGg2nrTYAm"
      },
      "source": [
        "## Counting Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WchynDMVnu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de3114c-48cd-4b54-907c-689dd8fd3df7"
      },
      "source": [
        "import nltk\n",
        "words = [\"men\", \"teacher\", \"men\", \"woman\"]\n",
        "FreqDist = nltk.FreqDist(words)\n",
        "for i,j in FreqDist.items():\n",
        "   print(i, \"---\", j)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "men --- 2\n",
            "teacher --- 1\n",
            "woman --- 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6au6fb1Tilk"
      },
      "source": [
        "## Word groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWR25NzQDtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893b4193-6daf-4249-a796-be1efb4bfde4"
      },
      "source": [
        "words = \"Learning python was such an amazing experience for me\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.bigrams(word_tokenize)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Learning', 'python'), ('python', 'was'), ('was', 'such'), ('such', 'an'), ('an', 'amazing'), ('amazing', 'experience'), ('experience', 'for'), ('for', 'me')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzrbKs2jRFfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd22487-ee95-43be-acba-272cc3e48af8"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.trigrams(word_tokenize)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Learning', 'python', 'was'), ('python', 'was', 'such'), ('was', 'such', 'an'), ('such', 'an', 'amazing'), ('an', 'amazing', 'experience'), ('amazing', 'experience', 'for'), ('experience', 'for', 'me')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj1BCGX_RX_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6780dd1-bc45-43aa-af56-b7429b90679a"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.ngrams(word_tokenize, 4)))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Learning', 'python', 'was', 'such'), ('python', 'was', 'such', 'an'), ('was', 'such', 'an', 'amazing'), ('such', 'an', 'amazing', 'experience'), ('an', 'amazing', 'experience', 'for'), ('amazing', 'experience', 'for', 'me')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BwByxESK4n"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mvsterSH2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d6de73d-a999-4df2-c73a-733db0a32219"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWOzz6ay2z-",
        "outputId": "7969993b-2dd6-45f3-8e5b-2fbcaf2b2aff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZiPb5cPSRAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db21c78d-17f8-440c-e455-1a5e697506a2"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Lem = WordNetLemmatizer()\n",
        "print(Lem.lemmatize(\"believes\"))\n",
        "print(Lem.lemmatize(\"retrieved\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "belief\n",
            "retrieved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgt9YaYISxMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0493c414-4276-4393-f2f3-69d7e4b128a0"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Lem = WordNetLemmatizer()\n",
        "print(Lem.lemmatize(\"believes\", pos=\"v\"))\n",
        "print(Lem.lemmatize(\"retrieved\", pos=\"v\"))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "believe\n",
            "retrieve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd30v8wvS8cM"
      },
      "source": [
        "## POS Taggers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Z59DxgTCaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8573bb-961a-4d79-f425-9588ecf4e2e0"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "words = \"we work here\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(nltk.pos_tag(word_tokenize))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('we', 'PRP'), ('work', 'VBP'), ('here', 'RB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGnMzBTVAn6"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hAhTQJaU--_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4c494d-2b44-43e7-889f-e394c25c4ce3"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NpKVSHWSfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d166fc-9b15-4a59-ceda-71adabf7082b"
      },
      "source": [
        "Text = \"tom is in london\"\n",
        "Tokenize = nltk.word_tokenize(Text)\n",
        "POS_tags = nltk.pos_tag(Tokenize)\n",
        "NameEn = nltk.ne_chunk(POS_tags)\n",
        "print(NameEn)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S tom/NN is/VBZ in/IN london/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekKMimU1YZ1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8ff196-8610-45ee-ed06-4aecf57e606e"
      },
      "source": [
        "!pip3 install textblob"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5DGtIvNYffo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04248e97-1e6c-4a72-8527-35032012180c"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Joe_Biden_Tweet = \"today is sunny\"\n",
        "Joe_Biden = TextBlob(Joe_Biden_Tweet)\n",
        "print(Joe_Biden.sentiment)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2gqbhMZqP8"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QWtSAyZqvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adbd70c8-2ebb-45b6-cc79-f3ac9afa838a"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Text = \"Smalle businesses neede relief\"\n",
        "spelling_mistakes = TextBlob(Text)\n",
        "print(spelling_mistakes.correct())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small business need relief\n"
          ]
        }
      ]
    }
  ]
}