{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_with_NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtao/dswebinar/blob/master/nlp/NLP_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMYbTJGjtMoz"
      },
      "source": [
        "# Natural Language Processing with NLTK\n",
        "\n",
        "[Jian Tao](https://coehpc.engr.tamu.edu/people/jian-tao/), Texas A&M University\n",
        "\n",
        "Sept 17, 2021\n",
        "\n",
        "Converted from \n",
        "\n",
        "**Intro to natural language processing with Python**\n",
        "\n",
        "Notebook by [Juan Cruz Martinez](https://livecodestream.dev/authors/bajcmartinez/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfKt9ernTGcK"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5aFgDKqOBI"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JpHHRXTKrI"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PwgHiH5qTQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c785d49a-b492-4082-ff1b-41752d9706a5"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAtuBOHr0Hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323b63d4-4ade-44b0-e5f4-3483233b3595"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "Text = \"Good morning, How you doing? Are you coming tonight?\"\n",
        "Tokenized = word_tokenize(Text)\n",
        "print(Tokenized)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'morning', ',', 'How', 'you', 'doing', '?', 'Are', 'you', 'coming', 'tonight', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZavEyf3shdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c9f59-7aba-40fe-ab08-80fd43e3cece"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "Text = \"Good morning, How you doing? Are you coming tonight?\"\n",
        "Tokenized = sent_tokenize(Text)\n",
        "print(Tokenized)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good morning, How you doing?', 'Are you coming tonight?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug23r1ziTRBB"
      },
      "source": [
        "## Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J_X7WZCtvSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8932142f-9931-4612-dff2-596044771e1e"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkFPhwmAt0_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745a1d82-a508-428f-e7e6-2a30a265a0ea"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "Text = [\"Good\", \"morning\", \"How\", \"you\", \"doing\", \"Are\", \"you\", \"coming\", \"tonight\"]\n",
        "for i in Text:\n",
        "   if i not in stopwords:\n",
        "       print(i)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good\n",
            "morning\n",
            "How\n",
            "Are\n",
            "coming\n",
            "tonight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbC3CKm-vTHs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "0c92cc1d-aff7-464f-c5a7-d897028c6922"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "','.join(stopwords)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i,me,my,myself,we,our,ours,ourselves,you,you're,you've,you'll,you'd,your,yours,yourself,yourselves,he,him,his,himself,she,she's,her,hers,herself,it,it's,its,itself,they,them,their,theirs,themselves,what,which,who,whom,this,that,that'll,these,those,am,is,are,was,were,be,been,being,have,has,had,having,do,does,did,doing,a,an,the,and,but,if,or,because,as,until,while,of,at,by,for,with,about,against,between,into,through,during,before,after,above,below,to,from,up,down,in,out,on,off,over,under,again,further,then,once,here,there,when,where,why,how,all,any,both,each,few,more,most,other,some,such,no,nor,not,only,own,same,so,than,too,very,s,t,can,will,just,don,don't,should,should've,now,d,ll,m,o,re,ve,y,ain,aren,aren't,couldn,couldn't,didn,didn't,doesn,doesn't,hadn,hadn't,hasn,hasn't,haven,haven't,isn,isn't,ma,mightn,mightn't,mustn,mustn't,needn,needn't,shan,shan't,shouldn,shouldn't,wasn,wasn't,weren,weren't,won,won't,wouldn,wouldn't\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDmzpqRTUdn"
      },
      "source": [
        "## Stemming Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI2Olf5asdTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2c2f62-48bb-4dc4-84a6-56a7a404b65d"
      },
      "source": [
        "help(nltk.stem)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package nltk.stem in nltk:\n",
            "\n",
            "NAME\n",
            "    nltk.stem - NLTK Stemmers\n",
            "\n",
            "DESCRIPTION\n",
            "    Interfaces used to remove morphological affixes from words, leaving\n",
            "    only the word stem.  Stemming algorithms aim to remove those affixes\n",
            "    required for eg. grammatical role, tense, derivational morphology\n",
            "    leaving only the stem of the word.  This is a difficult problem due to\n",
            "    irregular words (eg. common verbs in English), complicated\n",
            "    morphological rules, and part-of-speech and sense ambiguities\n",
            "    (eg. ``ceil-`` is not the stem of ``ceiling``).\n",
            "    \n",
            "    StemmerI defines a standard interface for stemmers.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    api\n",
            "    arlstem\n",
            "    arlstem2\n",
            "    cistem\n",
            "    isri\n",
            "    lancaster\n",
            "    porter\n",
            "    regexp\n",
            "    rslp\n",
            "    snowball\n",
            "    util\n",
            "    wordnet\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/nltk/stem/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st2JbM81u0RJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24312c0-2c10-49bf-e6e0-9bcbb714f3a1"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"Loving\", \"Chocolate\", \"Retrieving\"]\n",
        "for i in words:\n",
        "   print(ps.stem(i))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love\n",
            "chocol\n",
            "retriev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBjGg2nrTYAm"
      },
      "source": [
        "## Counting Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WchynDMVnu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22252980-7b2c-4566-fb96-2bf9c84b1aaa"
      },
      "source": [
        "import nltk\n",
        "words = [\"men\", \"teacher\", \"men\", \"woman\"]\n",
        "FreqDist = nltk.FreqDist(words)\n",
        "for i,j in FreqDist.items():\n",
        "   print(i, \"---\", j)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "men --- 2\n",
            "teacher --- 1\n",
            "woman --- 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6au6fb1Tilk"
      },
      "source": [
        "## Word groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWR25NzQDtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bd4194-e70e-45d9-e763-2f574de6d83a"
      },
      "source": [
        "words = \"Learning python was such an amazing experience for me\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.bigrams(word_tokenize)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Learning', 'python'), ('python', 'was'), ('was', 'such'), ('such', 'an'), ('an', 'amazing'), ('amazing', 'experience'), ('experience', 'for'), ('for', 'me')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzrbKs2jRFfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8b2e30-1a89-4927-9bf8-0d3ae372c3a4"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.trigrams(word_tokenize)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Learning', 'python', 'was'), ('python', 'was', 'such'), ('was', 'such', 'an'), ('such', 'an', 'amazing'), ('an', 'amazing', 'experience'), ('amazing', 'experience', 'for'), ('experience', 'for', 'me')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj1BCGX_RX_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da827ba7-c571-4745-df19-805d7e05f91b"
      },
      "source": [
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(list(nltk.ngrams(word_tokenize, 4)))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Learning', 'python', 'was', 'such'), ('python', 'was', 'such', 'an'), ('was', 'such', 'an', 'amazing'), ('such', 'an', 'amazing', 'experience'), ('an', 'amazing', 'experience', 'for'), ('amazing', 'experience', 'for', 'me')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BwByxESK4n"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3mvsterSH2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f223df1-a316-4a0c-f61d-bb97893e7e5a"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWOzz6ay2z-",
        "outputId": "49aaa6fa-042e-492d-b83c-099f3a3928f3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZiPb5cPSRAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489df437-edbf-4910-b995-c306801dd73e"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Lem = WordNetLemmatizer()\n",
        "print(Lem.lemmatize(\"believes\"))\n",
        "print(Lem.lemmatize(\"retrieved\"))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "belief\n",
            "retrieved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgt9YaYISxMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f3c921-dbdf-4877-9ec2-8dfaa7c0abd6"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "Lem = WordNetLemmatizer()\n",
        "print(Lem.lemmatize(\"believes\", pos=\"v\"))\n",
        "print(Lem.lemmatize(\"retrieved\", pos=\"v\"))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "believe\n",
            "retrieve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd30v8wvS8cM"
      },
      "source": [
        "## POS Taggers\n",
        "Exampls\n",
        "* PRP\tpersonal pronoun (hers, herself, him, himself)\n",
        "* RB\tadverb (occasionally, swiftly)\n",
        "* VBP\tverb, present tense not 3rd person singular(wrap)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Z59DxgTCaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d8ce09-5023-434a-963e-595fcc4c164e"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "words = \"we work here\"\n",
        "word_tokenize = nltk.word_tokenize(words)\n",
        "print(nltk.pos_tag(word_tokenize))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('we', 'PRP'), ('work', 'VBP'), ('here', 'RB')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vGnMzBTVAn6"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hAhTQJaU--_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e40d7b-5ed3-4b84-e8f8-bd0595d7e743"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NpKVSHWSfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba4cfd9-05de-4c73-e738-448d4ca911c4"
      },
      "source": [
        "Text = \"tom is in london\"\n",
        "Tokenize = nltk.word_tokenize(Text)\n",
        "POS_tags = nltk.pos_tag(Tokenize)\n",
        "NameEn = nltk.ne_chunk(POS_tags)\n",
        "print(NameEn)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S tom/NN is/VBZ in/IN london/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekKMimU1YZ1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce5732f-adae-4cc9-da55-e92cb451904b"
      },
      "source": [
        "!pip3 install textblob"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5DGtIvNYffo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154ce810-37dd-41bd-ea46-fc14a081c6fe"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Joe_Biden_Tweet = \"today is sunny\"\n",
        "Joe_Biden = TextBlob(Joe_Biden_Tweet)\n",
        "print(Joe_Biden.sentiment)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2gqbhMZqP8"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QWtSAyZqvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0c73c5-2fce-417c-ae74-d313324cba9c"
      },
      "source": [
        "from textblob import TextBlob\n",
        "Text = \"Smalle businesses neede relief\"\n",
        "spelling_mistakes = TextBlob(Text)\n",
        "print(spelling_mistakes.correct())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small business need relief\n"
          ]
        }
      ]
    }
  ]
}